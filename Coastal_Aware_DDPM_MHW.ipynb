{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee746ab",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸŒŠ Coastal-Aware SST Reconstruction + **Light DDPM** + Marine Heatwave Forecast (Bay of Bengal)\n",
    "\n",
    "Colab-ready notebook that demonstrates:\n",
    "- NOAA OISST download & subset\n",
    "- Coast-distance feature\n",
    "- **Coastal-Aware U-Net** for gap-filling + MC-Dropout uncertainty\n",
    "- **Light DDPM (patch-based)** with U-Net-style denoiser (fast demo: 20â€“50 steps)\n",
    "- Climatology (2010â€“2020) + **Hobday et al., 2016** MHW detection\n",
    "- Simple **3D CNN** short-range forecast (14â†’K days)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe5af5",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Install & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631dd359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip -q install xarray netCDF4 pandas numpy scipy matplotlib tqdm einops torch torchvision scikit-image\n",
    "\n",
    "import os, math, json, urllib.request, warnings, datetime as dt\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c5650",
   "metadata": {},
   "source": [
    "## ðŸ”§ User Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00391f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Year & forecast horizon\n",
    "YEAR_SELECT = 2019\n",
    "FUTURE_DAYS = 3\n",
    "assert FUTURE_DAYS in [1,3,5], \"FUTURE_DAYS must be one of {1,3,5}\"\n",
    "\n",
    "# Region (Bay of Bengal)\n",
    "REGION = dict(lon_min=80.0, lon_max=100.0, lat_min=5.0, lat_max=23.0)\n",
    "\n",
    "# Data years used to build training set for U-Net / diffusion / forecaster\n",
    "YEARS  = [2019]\n",
    "\n",
    "# Present-day plots (optional; may skip if slow)\n",
    "USE_PRESENT_DAY = True\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DATA_DIR = '/content/data'\n",
    "FIG_DIR  = '/content/figs'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# U-Net training (fast demo)\n",
    "PATCH  = 64\n",
    "STRIDE = 32\n",
    "EPOCHS = 6\n",
    "BATCH  = 32\n",
    "MC_T   = 6\n",
    "\n",
    "# Forecasting (fast demo): 14-day context -> 1-day model rolled K steps\n",
    "FCTX_DAYS = 14\n",
    "F_EPOCHS  = 3\n",
    "F_BATCH   = 32\n",
    "\n",
    "# Light DDPM (patch-based)\n",
    "DDPM_STEPS     = 40          # 20â€“50 recommended for quick demo\n",
    "DDPM_EPOCHS    = 3           # small epochs for speed\n",
    "DDPM_BATCH     = 32\n",
    "DDPM_BETA_SCHED= 'linear'    # 'linear' or 'cosine'\n",
    "DDPM_MIN_BETA  = 1e-4\n",
    "DDPM_MAX_BETA  = 0.02\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6146801",
   "metadata": {},
   "source": [
    "## 1) Download NOAA OISST (daily) & subset Bay of Bengal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf5c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_oisst_year(year:int):\n",
    "    url = f\"https://downloads.psl.noaa.gov/Datasets/noaa.oisst.v2.highres/sst.day.mean.{year}.nc\"\n",
    "    fn = f\"{DATA_DIR}/oisst_{year}.nc\"\n",
    "    if not os.path.exists(fn):\n",
    "        print('Downloading', url)\n",
    "        urllib.request.urlretrieve(url, fn)\n",
    "    return fn\n",
    "\n",
    "def open_subset_year(year:int):\n",
    "    fn = fetch_oisst_year(year)\n",
    "    ds = xr.open_dataset(fn, engine='netcdf4')\n",
    "    if 'sst' in ds.data_vars:  # normalize var name\n",
    "        ds = ds.rename({'sst':'SST'})\n",
    "    ds = ds.sel(lat=slice(REGION['lat_min'], REGION['lat_max']),\n",
    "                lon=slice(REGION['lon_min'], REGION['lon_max']))\n",
    "    if not np.issubdtype(ds['time'].dtype, np.datetime64):\n",
    "        ds = xr.decode_cf(ds)\n",
    "    return ds\n",
    "\n",
    "# Open years used for model training\n",
    "ds_list = [open_subset_year(y) for y in YEARS]\n",
    "ds = xr.concat(ds_list, dim='time').sortby('time')\n",
    "print(\"Dataset shape (time, lat, lon):\", ds['SST'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10660f2c",
   "metadata": {},
   "source": [
    "## 2) Coast Distance (0â€“100 km cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_sst = ds['SST'].astype('float32')\n",
    "land_mask = np.isnan(raw_sst.values)   # (T,H,W)\n",
    "land_any  = np.any(land_mask, axis=0)  # (H,W)\n",
    "ocean = ~land_any\n",
    "dist_pix = distance_transform_edt(ocean==1)\n",
    "\n",
    "lats = ds['lat'].values; lons = ds['lon'].values\n",
    "dlat_km = 111.0\n",
    "dlon_km = 111.0*np.cos(np.deg2rad(lats))\n",
    "dlon_km_grid = np.tile(dlon_km[:,None], (1, len(lons)))\n",
    "avg_step_km = np.sqrt((dlat_km**2 + dlon_km_grid**2)/2.0)  # RMS-ish coarse step\n",
    "dist_km = np.clip(dist_pix * avg_step_km, 0, 100)\n",
    "\n",
    "coast_dist = xr.DataArray(dist_km, coords={'lat': ds['lat'], 'lon': ds['lon']},\n",
    "                          dims=('lat','lon'), name='coast_km')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be468ddd",
   "metadata": {},
   "source": [
    "## 3) Normalize & Patchify (simulate gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arr = raw_sst.copy().astype('float32').fillna(0.0).clip(min=-2.0, max=35.0)\n",
    "mu = float(arr.mean()); sigma = float(arr.std())\n",
    "arrn = (arr - mu) / (sigma + 1e-6)\n",
    "coast01 = (coast_dist/100.0).clip(0,1)\n",
    "\n",
    "def extract_patches_with_coast(da3d, coast2d, PATCH=64, STRIDE=32):\n",
    "    A = da3d.values.astype('float32')   # (T,H,W)\n",
    "    C = coast2d.values.astype('float32')# (H,W)\n",
    "    T,H,W = A.shape\n",
    "    X, coast, times = [], [], []\n",
    "    for t in range(T):\n",
    "        F = A[t]\n",
    "        for i in range(0, H-PATCH+1, STRIDE):\n",
    "            for j in range(0, W-PATCH+1, STRIDE):\n",
    "                X.append(F[i:i+PATCH, j:j+PATCH])\n",
    "                coast.append(C[i:i+PATCH, j:j+PATCH])\n",
    "                times.append(t)\n",
    "    X = np.stack(X)[:,None,...]        # (N,1,P,P)\n",
    "    coast = np.stack(coast)[:,None,...]# (N,1,P,P)\n",
    "    return X.astype('float32'), coast.astype('float32'), np.array(times)\n",
    "\n",
    "X_all, C_all, times_idx = extract_patches_with_coast(arrn, coast01, PATCH=PATCH, STRIDE=STRIDE)\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "# keep-prob mask (1 = keep/original, 0 = hole) â€“ we'll provide mask as conditioning\n",
    "M_all = (rng.random(X_all.shape) < 0.8).astype('float32')\n",
    "Y_all = X_all.copy()\n",
    "X_in  = X_all * M_all\n",
    "\n",
    "# train/val/test split\n",
    "N = len(X_in); perm = rng.permutation(N)\n",
    "ntr, nva = int(0.7*N), int(0.85*N)\n",
    "tr_idx, va_idx, te_idx = perm[:ntr], perm[ntr:nva], perm[nva:]\n",
    "\n",
    "Xtr, Mtr, Ctr, Ytr = X_in[tr_idx], M_all[tr_idx], C_all[tr_idx], Y_all[tr_idx]\n",
    "Xva, Mva, Cva, Yva = X_in[va_idx], M_all[va_idx], C_all[va_idx], Y_all[va_idx]\n",
    "Xte, Mte, Cte, Yte = X_in[te_idx], M_all[te_idx], C_all[te_idx], Y_all[te_idx]\n",
    "\n",
    "print(\"Train/Val/Test:\", Xtr.shape, Xva.shape, Xte.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feca01e",
   "metadata": {},
   "source": [
    "## 4) Coastal-Aware U-Net (reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_out, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(c_out, c_out, 3, padding=1), nn.ReLU(),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "    def forward(self,x): return self.conv(x)\n",
    "\n",
    "class UNetCoastal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 3 in-channels: masked SST, mask, coast01\n",
    "        self.enc1 = ConvBlock(3, 32)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBlock(32, 64)\n",
    "        self.enc3 = ConvBlock(64, 128)\n",
    "        self.dec3 = ConvBlock(128+64, 64)\n",
    "        self.dec2 = ConvBlock(64+32, 32)\n",
    "        self.outc = nn.Conv2d(32, 1, 1)\n",
    "    def forward(self, x, m, c):\n",
    "        z = torch.cat([x, m, c], dim=1)\n",
    "        e1 = self.enc1(z)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        d3 = F.interpolate(e3, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e2], dim=1))\n",
    "        d2 = F.interpolate(d3, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e1], dim=1))\n",
    "        y  = self.outc(d2)\n",
    "        return y\n",
    "\n",
    "model = UNetCoastal().to(DEVICE)\n",
    "\n",
    "class SSTDset(Dataset):\n",
    "    def __init__(self, X,M,C,Y): self.X,self.M,self.C,self.Y = X,M,C,Y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self,i):\n",
    "        return (torch.from_numpy(self.X[i]),\n",
    "                torch.from_numpy(self.M[i]),\n",
    "                torch.from_numpy(self.C[i]),\n",
    "                torch.from_numpy(self.Y[i]))\n",
    "\n",
    "train_dl = DataLoader(SSTDset(Xtr,Mtr,Ctr,Ytr), batch_size=BATCH, shuffle=True, drop_last=False)\n",
    "val_dl   = DataLoader(SSTDset(Xva,Mva,Cva,Yva), batch_size=BATCH, shuffle=False, drop_last=False)\n",
    "\n",
    "opt  = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "def run_epoch_unet(dl, train=True):\n",
    "    model.train(train)\n",
    "    tot, n = 0.0, 0\n",
    "    for xb,mb,cb,yb in dl:\n",
    "        xb,mb,cb,yb = xb.to(DEVICE), mb.to(DEVICE), cb.to(DEVICE), yb.to(DEVICE)\n",
    "        pred = model(xb,mb,cb)\n",
    "        loss = F.l1_loss(pred,yb) + 0.2*F.mse_loss(pred,yb)\n",
    "        if train:\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "        tot += loss.item()*len(xb); n += len(xb)\n",
    "    return tot/max(n,1)\n",
    "\n",
    "print(\"Training U-Net (quick)...\")\n",
    "best=float('inf'); best_w=None\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = run_epoch_unet(train_dl, True)\n",
    "    va = run_epoch_unet(val_dl,   False)\n",
    "    print(f\"U-Net ep {ep:02d} | train {tr:.4f} | val {va:.4f}\")\n",
    "    if va<best: best=va; best_w={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "if best_w is not None: model.load_state_dict(best_w); print(\"U-Net best val:\", best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529375e6",
   "metadata": {},
   "source": [
    "## 5) Evaluate & MC-Dropout Uncertainty (U-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af4b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dl = DataLoader(SSTDset(Xte,Mte,Cte,Yte), batch_size=BATCH, shuffle=False)\n",
    "\n",
    "def grad_mag(z):\n",
    "    gx = z[...,1:] - z[...,:-1]\n",
    "    gy = z[:,:,1:,:] - z[:,:,:-1,:]\n",
    "    gx = F.pad(gx,(0,1,0,0)); gy = F.pad(gy,(0,0,0,1))\n",
    "    return torch.sqrt(gx**2 + gy**2 + 1e-8)\n",
    "\n",
    "def eval_unet(dl, T_mc=MC_T):\n",
    "    model.train(True)  # keep dropout ON\n",
    "    maes,rmses,gdiffs=[],[],[]\n",
    "    with torch.no_grad():\n",
    "        for xb,mb,cb,yb in dl:\n",
    "            xb,mb,cb,yb = xb.to(DEVICE), mb.to(DEVICE), cb.to(DEVICE), yb.to(DEVICE)\n",
    "            preds=[model(xb,mb,cb) for _ in range(T_mc)]\n",
    "            pred=torch.stack(preds,0).mean(0)\n",
    "            mae = F.l1_loss(pred,yb).item()\n",
    "            rmse= torch.sqrt(F.mse_loss(pred,yb)).item()\n",
    "            gdf = F.l1_loss(grad_mag(pred),grad_mag(yb)).item()\n",
    "            maes.append(mae); rmses.append(rmse); gdiffs.append(gdf)\n",
    "    return float(np.mean(maes)), float(np.mean(rmses)), float(np.mean(gdiffs))\n",
    "\n",
    "mae_u, rmse_u, g_u = eval_unet(test_dl)\n",
    "print({\"U-Net MAE\":mae_u, \"U-Net RMSE\":rmse_u, \"U-Net GradDiff\":g_u})\n",
    "\n",
    "def denorm(z): return z*(sigma+1e-6)+mu\n",
    "\n",
    "# Visualize one sample\n",
    "i = np.random.randint(0, len(Xte))\n",
    "xb = torch.from_numpy(Xte[i:i+1]).to(DEVICE)\n",
    "mb = torch.from_numpy(Mte[i:i+1]).to(DEVICE)\n",
    "cb = torch.from_numpy(Cte[i:i+1]).to(DEVICE)\n",
    "yb = torch.from_numpy(Yte[i:i+1]).to(DEVICE)\n",
    "\n",
    "model.train(True)\n",
    "preds=[model(xb,mb,cb) for _ in range(MC_T)]\n",
    "pm = torch.stack(preds,0).mean(0)[0,0].detach().cpu().numpy()\n",
    "ps = torch.stack(preds,0).std(0)[0,0].detach().cpu().numpy()\n",
    "gt = yb[0,0].detach().cpu().numpy()\n",
    "ms = mb[0,0].detach().cpu().numpy()\n",
    "\n",
    "fig,axs=plt.subplots(1,4,figsize=(14,3.2))\n",
    "axs[0].imshow(denorm(gt),origin='lower'); axs[0].set_title('GT Â°C')\n",
    "axs[1].imshow(ms,origin='lower'); axs[1].set_title('Mask')\n",
    "axs[2].imshow(denorm(pm),origin='lower'); axs[2].set_title('U-Net Pred Â°C')\n",
    "im=axs[3].imshow(ps*(sigma+1e-6),origin='lower'); axs[3].set_title('Uncertainty Â°C')\n",
    "plt.colorbar(im,ax=axs[3]); plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/unet_recon_uncertainty.png\",dpi=200); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b0adc",
   "metadata": {},
   "source": [
    "## 6) Light DDPM (Patch-based) â€” U-Net-like denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec7b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_beta_schedule(T:int, schedule='linear', beta_start=1e-4, beta_end=0.02):\n",
    "    if schedule=='linear':\n",
    "        return torch.linspace(beta_start, beta_end, T)\n",
    "    elif schedule=='cosine':\n",
    "        s = 0.008\n",
    "        t = torch.linspace(0, 1, T+1)\n",
    "        alphas_cumprod = (torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2)\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        return torch.clip(betas, 1e-4, 0.02)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown schedule\")\n",
    "\n",
    "BETAS = make_beta_schedule(DDPM_STEPS, DDPM_BETA_SCHED, DDPM_MIN_BETA, DDPM_MAX_BETA).to(DEVICE)\n",
    "ALPHAS = (1.0 - BETAS)\n",
    "ALPHAS_BAR = torch.cumprod(ALPHAS, dim=0)  # (T,)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    out = a.gather(0, t).float()\n",
    "    while len(out.shape) < len(x_shape):\n",
    "        out = out.unsqueeze(-1)\n",
    "    return out\n",
    "\n",
    "class DDPMDenoiser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4 channels: x_t, mask, coast, t_map\n",
    "        self.enc1 = ConvBlock(4, 32)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBlock(32, 64)\n",
    "        self.enc3 = ConvBlock(64, 128)\n",
    "        self.dec3 = ConvBlock(128+64, 64)\n",
    "        self.dec2 = ConvBlock(64+32, 32)\n",
    "        self.outc = nn.Conv2d(32, 1, 1)\n",
    "    def forward(self, x_t, m, c, t):\n",
    "        B, _, H, W = x_t.shape\n",
    "        t_norm = (t.float()/ (DDPM_STEPS-1)).view(B,1,1,1).expand(B,1,H,W)\n",
    "        z = torch.cat([x_t, m, c, t_norm], dim=1)\n",
    "        e1 = self.enc1(z)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        d3 = F.interpolate(e3, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        d3 = self.dec3(torch.cat([d3, e2], dim=1))\n",
    "        d2 = F.interpolate(d3, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([d2, e1], dim=1))\n",
    "        eps = self.outc(d2)\n",
    "        return eps\n",
    "\n",
    "ddpm = DDPMDenoiser().to(DEVICE)\n",
    "opt_d = torch.optim.AdamW(ddpm.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "class PatchSet(Dataset):\n",
    "    def __init__(self, X, M, C, Y):\n",
    "        self.X, self.M, self.C, self.Y = X, M, C, Y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self,i):\n",
    "        return (torch.from_numpy(self.X[i]),\n",
    "                torch.from_numpy(self.M[i]),\n",
    "                torch.from_numpy(self.C[i]),\n",
    "                torch.from_numpy(self.Y[i]))\n",
    "\n",
    "tr_dl_ddpm = DataLoader(PatchSet(Xtr,Mtr,Ctr,Ytr), batch_size=DDPM_BATCH, shuffle=True)\n",
    "va_dl_ddpm = DataLoader(PatchSet(Xva,Mva,Cva,Yva), batch_size=DDPM_BATCH, shuffle=False)\n",
    "\n",
    "def q_sample(x0, t, noise):\n",
    "    sqrt_ab = torch.sqrt(extract(ALPHAS_BAR, t, x0.shape))\n",
    "    sqrt_one_minus_ab = torch.sqrt(1.0 - extract(ALPHAS_BAR, t, x0.shape))\n",
    "    return sqrt_ab * x0 + sqrt_one_minus_ab * noise\n",
    "\n",
    "def train_ddpm_epoch(dl):\n",
    "    ddpm.train(True)\n",
    "    total, n = 0.0, 0\n",
    "    for xb, mb, cb, yb in dl:\n",
    "        xb, mb, cb, yb = xb.to(DEVICE), mb.to(DEVICE), cb.to(DEVICE), yb.to(DEVICE)\n",
    "        B = yb.size(0)\n",
    "        t = torch.randint(0, DDPM_STEPS, (B,), device=DEVICE).long()\n",
    "        noise = torch.randn_like(yb)\n",
    "        x_t = q_sample(yb, t, noise)\n",
    "        eps_hat = ddpm(x_t, mb, cb, t)\n",
    "        loss = F.mse_loss(eps_hat, noise)\n",
    "        opt_d.zero_grad(); loss.backward(); opt_d.step()\n",
    "        total += loss.item()*B; n += B\n",
    "    return total/max(n,1)\n",
    "\n",
    "def eval_ddpm_epoch(dl):\n",
    "    ddpm.train(False)\n",
    "    total, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, mb, cb, yb in dl:\n",
    "            xb, mb, cb, yb = xb.to(DEVICE), mb.to(DEVICE), cb.to(DEVICE), yb.to(DEVICE)\n",
    "            B = yb.size(0)\n",
    "            t = torch.randint(0, DDPM_STEPS, (B,), device=DEVICE).long()\n",
    "            noise = torch.randn_like(yb)\n",
    "            x_t = q_sample(yb, t, noise)\n",
    "            eps_hat = ddpm(x_t, mb, cb, t)\n",
    "            loss = F.mse_loss(eps_hat, noise)\n",
    "            total += loss.item()*B; n += B\n",
    "    return total/max(n,1)\n",
    "\n",
    "print(\"Training Light DDPM (patch-based, quick)...\")\n",
    "best_d=float('inf'); best_dw=None\n",
    "for ep in range(1, DDPM_EPOCHS+1):\n",
    "    tr = train_ddpm_epoch(tr_dl_ddpm)\n",
    "    va = eval_ddpm_epoch(va_dl_ddpm)\n",
    "    print(f\"DDPM ep {ep:02d} | train {tr:.5f} | val {va:.5f}\")\n",
    "    if va < best_d: best_d = va; best_dw = {k:v.detach().cpu().clone() for k,v in ddpm.state_dict().items()}\n",
    "if best_dw is not None:\n",
    "    ddpm.load_state_dict(best_dw)\n",
    "    print(\"DDPM best val:\", best_d)\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_step(x_t, m, c, t):\n",
    "    beta_t     = extract(BETAS, t, x_t.shape)\n",
    "    alpha_t    = extract(ALPHAS, t, x_t.shape)\n",
    "    alpha_bar_t= extract(ALPHAS_BAR, t, x_t.shape)\n",
    "    eps_hat = ddpm(x_t, m, c, t)\n",
    "    mean = (1.0/torch.sqrt(alpha_t))*(x_t - ((1.0 - alpha_t)/torch.sqrt(1.0 - alpha_bar_t))*eps_hat)\n",
    "    if t.min() > 0:\n",
    "        z = torch.randn_like(x_t)\n",
    "        sigma_t = torch.sqrt(beta_t)\n",
    "        x_prev = mean + sigma_t * z\n",
    "    else:\n",
    "        x_prev = mean\n",
    "    return x_prev\n",
    "\n",
    "@torch.no_grad()\n",
    "def ddpm_sample(m, c, shape):\n",
    "    B = m.size(0)\n",
    "    x_t = torch.randn(shape, device=DEVICE)\n",
    "    for step in reversed(range(DDPM_STEPS)):\n",
    "        t = torch.full((B,), step, device=DEVICE, dtype=torch.long)\n",
    "        x_t = p_sample_step(x_t, m, c, t)\n",
    "    return x_t  # normalized units\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc01a1f",
   "metadata": {},
   "source": [
    "## 7) Compare Reconstructions: U-Net vs DDPM (patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b96809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# take a random test patch\n",
    "i = np.random.randint(0, len(Xte))\n",
    "xb = torch.from_numpy(Xte[i:i+1]).to(DEVICE)   # masked input (not directly used by DDPM trainer)\n",
    "mb = torch.from_numpy(Mte[i:i+1]).to(DEVICE)   # mask channel\n",
    "cb = torch.from_numpy(Cte[i:i+1]).to(DEVICE)   # coast channel\n",
    "yb = torch.from_numpy(Yte[i:i+1]).to(DEVICE)   # GT (normalized)\n",
    "\n",
    "# U-Net MC average\n",
    "model.train(True)  # keep dropout for MC\n",
    "preds=[model(xb,mb,cb) for _ in range(MC_T)]\n",
    "pm_unet = torch.stack(preds,0).mean(0)[0,0].detach().cpu().numpy()\n",
    "mask_np = mb[0,0].detach().cpu().numpy()\n",
    "gt_np   = yb[0,0].detach().cpu().numpy()\n",
    "\n",
    "# DDPM conditional sample\n",
    "ddpm.eval()\n",
    "x_gen = ddpm_sample(mb, cb, shape=yb.shape)    # normalized\n",
    "pm_ddpm = x_gen[0,0].detach().cpu().numpy()\n",
    "\n",
    "# plot\n",
    "def denorm(z): return z*(sigma+1e-6)+mu\n",
    "fig,axs=plt.subplots(1,4,figsize=(15,3.4))\n",
    "axs[0].imshow(denorm(gt_np), origin='lower'); axs[0].set_title('GT Â°C')\n",
    "axs[1].imshow(denorm((xb[0,0].cpu().numpy())), origin='lower'); axs[1].set_title('Masked In Â°C')\n",
    "axs[2].imshow(denorm(pm_unet), origin='lower'); axs[2].set_title('U-Net Recon Â°C')\n",
    "axs[3].imshow(denorm(pm_ddpm), origin='lower'); axs[3].set_title('DDPM Sample Â°C')\n",
    "for ax in axs: ax.axis('off')\n",
    "plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/compare_unet_ddpm.png\", dpi=200); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3535eab",
   "metadata": {},
   "source": [
    "## 8) Climatology (2010â€“2020) & MHW Detector (Hobday et al., 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb2395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_years = list(range(2010, 2021))\n",
    "base_list = []\n",
    "print(\"Building climatology (may download multiple yearly files) ...\")\n",
    "for y in base_years:\n",
    "    ds_y = open_subset_year(y)\n",
    "    base_list.append(ds_y)\n",
    "clim_ds = xr.concat(base_list, dim='time').sortby('time')\n",
    "\n",
    "clim = clim_ds['SST'].groupby('time.dayofyear').mean('time', skipna=True)\n",
    "clim = clim.rolling(dayofyear=11, center=True, min_periods=1).mean()\n",
    "\n",
    "thresh = clim_ds['SST'].groupby('time.dayofyear').quantile(0.90, dim='time', skipna=True)\n",
    "thresh = thresh.rolling(dayofyear=11, center=True, min_periods=1).mean()\n",
    "\n",
    "def detect_mhw(ds_year, clim, thresh):\n",
    "    da = ds_year['SST'] if 'SST' in ds_year else ds_year\n",
    "    if not np.issubdtype(da['time'].dtype, np.datetime64):\n",
    "        da = xr.decode_cf(ds_year)['SST']\n",
    "    doy = da['time'].dt.dayofyear\n",
    "    clim_match   = clim.sel(dayofyear=doy)\n",
    "    thresh_match = thresh.sel(dayofyear=doy)\n",
    "    anomaly = da - clim_match\n",
    "    mhw = anomaly.where(da > thresh_match)\n",
    "    return mhw, anomaly, thresh_match\n",
    "\n",
    "def longest_consecutive_run(binary_series):\n",
    "    best = cur = 0\n",
    "    for v in binary_series:\n",
    "        if v: cur += 1; best = max(best, cur)\n",
    "        else: cur = 0\n",
    "    return best\n",
    "\n",
    "print(f\"Analyzing MHW for {YEAR_SELECT} ...\")\n",
    "ds_y = open_subset_year(YEAR_SELECT)\n",
    "mhw, anomaly, thresh_match = detect_mhw(ds_y, clim, thresh)\n",
    "\n",
    "area_frac_per_day = mhw.notnull().mean(dim=(\"lat\",\"lon\")).fillna(0.0)\n",
    "total_days = float((area_frac_per_day > 0).sum().item())\n",
    "longest = int(longest_consecutive_run((area_frac_per_day.values > 0.2).astype(np.int32)))\n",
    "mean_intensity = float(mhw.mean(skipna=True).values)\n",
    "print(f\"{YEAR_SELECT} summary: total_days={total_days:.0f}, longest_run_days={longest}, mean_intensity={mean_intensity:.2f}Â°C\")\n",
    "\n",
    "has_mhw = (area_frac_per_day.values > 0).any()\n",
    "if has_mhw:\n",
    "    first_idx = int(np.argmax(area_frac_per_day.values > 0))\n",
    "    first_day = pd.to_datetime(ds_y.time.values[first_idx]).strftime(\"%Y-%m-%d\")\n",
    "    plt.figure(figsize=(6,4))\n",
    "    mhw.isel(time=first_idx).plot(cmap=\"hot\", vmin=0)\n",
    "    plt.title(f\"MHW anomaly map â€” {YEAR_SELECT} â€” {first_day}\")\n",
    "    plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/MHW_map_{YEAR_SELECT}.png\", dpi=200); plt.show()\n",
    "else:\n",
    "    print(f\"No MHW days found in {YEAR_SELECT} (90th percentile threshold).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d045ff",
   "metadata": {},
   "source": [
    "## 9) Present-day anomaly & MHW map (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daffdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if USE_PRESENT_DAY:\n",
    "    try:\n",
    "        cur_year = dt.date.today().year\n",
    "        ds_today = open_subset_year(cur_year)\n",
    "        latest_time = str(ds_today.time.values[-1])[:10]\n",
    "        mhw_today, anomaly_today, thresh_today = detect_mhw(ds_today, clim, thresh)\n",
    "\n",
    "        plt.figure(figsize=(10,4))\n",
    "        anomaly_today.isel(time=-1).plot(cmap=\"RdBu_r\", vmin=-3, vmax=3)\n",
    "        plt.title(f\"SST Anomaly â€” {latest_time}\")\n",
    "        plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/SST_anomaly_present.png\", dpi=200); plt.show()\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        mhw_today.isel(time=-1).plot(cmap=\"hot\", vmin=0)\n",
    "        plt.title(f\"MHW Mask â€” {latest_time}\")\n",
    "        plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/MHW_present.png\", dpi=200); plt.show()\n",
    "        print('Present-day processed:', latest_time)\n",
    "    except Exception as e:\n",
    "        print('âš  Present-day step skipped:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d20be",
   "metadata": {},
   "source": [
    "## 10) Short-range Forecast (3D CNN): past 14 â†’ next day, rolled K steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a94fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = arrn.values.astype('float32')  # normalized (T,H,W) for YEARS\n",
    "T,H,W = A.shape\n",
    "T_in = FCTX_DAYS\n",
    "\n",
    "def build_sequences(A, T_in=14):\n",
    "    Xseq, Yt = [], []\n",
    "    for t in range(T_in, len(A)-1):\n",
    "        Xseq.append(A[t-T_in:t])\n",
    "        Yt.append(A[t+1])\n",
    "    if len(Xseq)==0:\n",
    "        raise RuntimeError(\"Not enough days to build sequences. Increase YEARS or reduce FCTX_DAYS.\")\n",
    "    Xseq = np.stack(Xseq)[:,None,...]  # (N,1,T_in,H,W)\n",
    "    Yt   = np.stack(Yt)[:,None,...]    # (N,1,H,W)\n",
    "    return Xseq.astype('float32'), Yt.astype('float32')\n",
    "\n",
    "Xseq, Yseq = build_sequences(A, T_in=T_in)\n",
    "N = len(Xseq); idx = np.arange(N)\n",
    "rng = np.random.default_rng(SEED); rng.shuffle(idx)\n",
    "ntr = int(0.8*N)\n",
    "tr_idx, va_idx = idx[:ntr], idx[ntr:] if N-ntr>0 else (idx, idx)\n",
    "Xtr_s, Ytr_s = Xseq[tr_idx], Yseq[tr_idx]\n",
    "Xva_s, Yva_s = Xseq[va_idx], Yseq[va_idx]\n",
    "\n",
    "class CNN3DForecaster(nn.Module):\n",
    "    def __init__(self, T_in=14):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv3d(1, 8, kernel_size=(3,3,3), padding=(1,1,1)), nn.ReLU(),\n",
    "            nn.Conv3d(8,16, kernel_size=(3,3,3), padding=(1,1,1)), nn.ReLU(),\n",
    "            nn.Conv3d(16,16, kernel_size=(3,3,3), padding=(1,1,1)), nn.ReLU()\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv3d(16, 8, kernel_size=(T_in,1,1)), nn.ReLU(),\n",
    "            nn.Conv3d(8, 1, kernel_size=1)\n",
    "        )\n",
    "    def forward(self, x):  # (B,1,T,H,W)\n",
    "        z = self.net(x)\n",
    "        y = self.head(z)   # (B,1,1,H,W)\n",
    "        return y.squeeze(2)\n",
    "\n",
    "f_model = CNN3DForecaster(T_in=T_in).to(DEVICE)\n",
    "optf = torch.optim.AdamW(f_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "class SeqSet(Dataset):\n",
    "    def __init__(self, X,Y): self.X,self.Y = X,Y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self,i): return torch.from_numpy(self.X[i]), torch.from_numpy(self.Y[i])\n",
    "\n",
    "tr_dl_s = DataLoader(SeqSet(Xtr_s, Ytr_s), batch_size=F_BATCH, shuffle=True)\n",
    "va_dl_s = DataLoader(SeqSet(Xva_s, Yva_s), batch_size=F_BATCH, shuffle=False)\n",
    "\n",
    "def run_epoch_fore(dl, train=True):\n",
    "    f_model.train(train)\n",
    "    tot, n = 0.0, 0\n",
    "    for xb,yb in dl:\n",
    "        xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        pred = f_model(xb)\n",
    "        loss = F.l1_loss(pred,yb) + 0.2*F.mse_loss(pred,yb)\n",
    "        if train: optf.zero_grad(); loss.backward(); optf.step()\n",
    "        tot += loss.item()*len(xb); n += len(xb)\n",
    "    return tot/max(n,1)\n",
    "\n",
    "print(\"Training forecaster (quick)...\")\n",
    "bestf=float('inf'); bestfw=None\n",
    "for ep in range(1, F_EPOCHS+1):\n",
    "    tr = run_epoch_fore(tr_dl_s, True)\n",
    "    va = run_epoch_fore(va_dl_s, False)\n",
    "    print(f\"[Forecast] ep {ep:02d} | train {tr:.4f} | val {va:.4f}\")\n",
    "    if va<bestf: bestf=va; bestfw={k:v.detach().cpu().clone() for k,v in f_model.state_dict().items()}\n",
    "if bestfw is not None:\n",
    "    f_model.load_state_dict(bestfw)\n",
    "    print(\"Forecast best val:\", bestf)\n",
    "\n",
    "@torch.no_grad()\n",
    "def forecast_multi(model, A_last, steps=3, T_in=FCTX_DAYS):\n",
    "    preds = []\n",
    "    seq = A_last.copy()\n",
    "    for _ in range(steps):\n",
    "        x = torch.from_numpy(seq[None,None,...]).to(DEVICE)\n",
    "        yhat = model(x).cpu().numpy()[0,0]\n",
    "        preds.append(yhat)\n",
    "        seq = np.concatenate([seq[1:], yhat[None,...]], axis=0)\n",
    "    return np.array(preds)\n",
    "\n",
    "# build last-seq from selected year\n",
    "ds_y = open_subset_year(YEAR_SELECT)\n",
    "sst_y = ds_y['SST'].astype('float32').fillna(0.0).clip(min=-2.0, max=35.0)\n",
    "A_year = ((sst_y - mu) / (sigma + 1e-6)).values.astype('float32')\n",
    "if A_year.shape[0] < FCTX_DAYS+1:\n",
    "    raise RuntimeError(\"Chosen year lacks days; reduce FCTX_DAYS or choose another year.\")\n",
    "\n",
    "last_seq = A_year[-FCTX_DAYS:]  # (14,H,W)\n",
    "preds_n = forecast_multi(f_model, last_seq, steps=int(FUTURE_DAYS), T_in=FCTX_DAYS)\n",
    "preds_c = preds_n*(sigma+1e-6)+mu\n",
    "\n",
    "last_date = pd.to_datetime(ds_y.time.values[-1])\n",
    "future_times = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=int(FUTURE_DAYS), freq='D')\n",
    "\n",
    "pred_da = xr.DataArray(preds_c, coords={'time': future_times, 'lat': ds_y['lat'], 'lon': ds_y['lon']},\n",
    "                       dims=('time','lat','lon'), name='SST_pred')\n",
    "\n",
    "# detect MHW on forecast fields\n",
    "mhw_list = []; anom_list=[]\n",
    "for tt in range(int(FUTURE_DAYS)):\n",
    "    date = future_times[tt]; doy  = int(date.dayofyear)\n",
    "    clim_d = clim.sel(dayofyear=doy, method='nearest')\n",
    "    thr_d  = thresh.sel(dayofyear=doy, method='nearest')\n",
    "    sst_d  = pred_da.isel(time=tt)\n",
    "    anom_d = sst_d - clim_d\n",
    "    mhw_d  = anom_d.where(sst_d > thr_d)\n",
    "    anom_list.append(anom_d); mhw_list.append(mhw_d)\n",
    "\n",
    "anom_future = xr.concat(anom_list, dim='time'); anom_future['time'] = future_times\n",
    "mhw_future  = xr.concat(mhw_list,  dim='time'); mhw_future['time']  = future_times\n",
    "\n",
    "for i, t in enumerate(future_times):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1); pred_da.isel(time=i).plot(cmap='jet'); plt.title(f'Forecast SST (Â°C)\\n{t.date()}')\n",
    "    plt.subplot(1,3,2); anom_future.isel(time=i).plot(cmap='RdBu_r', vmin=-3, vmax=3); plt.title('Forecast Anomaly (Â°C)')\n",
    "    plt.subplot(1,3,3); mhw_future.isel(time=i).plot(cmap='hot', vmin=0); plt.title('Forecast MHW mask')\n",
    "    plt.tight_layout(); plt.savefig(f\"{FIG_DIR}/forecast_{YEAR_SELECT}_day{i+1}.png\", dpi=200); plt.show()\n",
    "\n",
    "area_frac = mhw_future.notnull().mean(dim=('lat','lon')).fillna(0.0).to_pandas()\n",
    "summary = pd.DataFrame({'date': future_times, 'mhw_area_fraction': area_frac.values})\n",
    "summary.to_csv(f\"{FIG_DIR}/forecast_MHW_summary_{YEAR_SELECT}.csv\", index=False)\n",
    "print(\"Saved:\")\n",
    "print(f\" - Comparison plot: {FIG_DIR}/compare_unet_ddpm.png\")\n",
    "print(f\" - MHW map (example year): {FIG_DIR}/MHW_map_{YEAR_SELECT}.png (if any)\")\n",
    "print(f\" - Forecast per-day: {FIG_DIR}/forecast_{YEAR_SELECT}_day*.png\")\n",
    "print(f\" - Forecast summary CSV: {FIG_DIR}/forecast_MHW_summary_{YEAR_SELECT}.csv\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
